{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    数据先经过encoder层进行压缩提取特称，再经过decoder层进行解压缩得到与原始输入数据尽可能相似的数据（通过输入与输出对比使得loss尽可能小），应用有图像重建，语义分割，机器翻译等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greddy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    greddy search类似只有一条路走到底，取每个时间点概率最大的词。会出现的问题是在t0时间取最大概率词w1并在此情况下在t1时间取最大概率词w2的连乘概率可能小于在t0时间取非最大概率词w1‘并在此情况下在t1时间取词w2’的连乘概率，即每个时间点依次取最大概率组成的一句话总的概率可能不是最大的；\n",
    "    beam search会在t0时间取概率最大的k个词，并在t1时间分别对这k个词输出与之对应的k个词，在接下来的时间也如此，直到遇到eos结尾标志，回溯取该条路线的所有词组成的一句话"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    为了处理长程联系与信息传递冗余问题，每一个decoder时间点分别对每一个encoder时间点输出分配注意力权重（根据encoder输出与隐状态，每一个decoder对应的各个encoder时间点通过softmax计算），意思为每一个decoder时间点对各个encoder的信息分别需要多大的关注度，然后将各个encoder输出乘各自的注意力权重得到每一个decoder时间点的context vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    每个词对应的embeding向量都是唯一确定的，不能很好解决一个词在不同句子里面的一词多义问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ELMo 由两层双向循环网络组成，每个时间点的词有初始embeding为h0，经过第一层双向网络拼接得到第一层的embeding为h1，再由输入与第一层输出经过第二层双向网络后拼接得到第二层的embeding为h2，则这个词经过ELMo后所得到的的新embeding为h0，h1，h2的加权求和，其中加权系数为所要学习的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    对句子中的词可以并行计算，不依赖顺序计算；由于self-attention可以很好地处理句子中词的远程联系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    layer normalization是对一个batch中的一句话的每个词的embeding做标准化，在NLP工程中有实际意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    因为Transformer中虽然可以将输入的词与词之间相互的影响力联系起来，但是不能很好的反映词与词之间的位置关系也就是语序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    self-attention将输入embeding经过参数矩阵得到Wk，Wq，Wv，将每个词对应的q分别与自己以及其他词的k相乘得到score并标准化，对这些词的score再进行softmax得到每个词对应的权重，则每个词新的embeding表示为各个词的v加权求和z\n",
    "    multi-head attention有多个参数矩阵得到多个Wk，Wq，Wv，最后每个词新的embeding为多个加权求和z拼接起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    输入与经过masked multi self-attention的输出相加，再layer normalization，得到的结果与经过全连接层的输出相加，再layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    文本分类将句子输入GPT最后经过全连接\n",
    "    文本关系将前提与假说输入GPT最后经过全连接\n",
    "    文本相似度将两个句子分别输入GPT加和后经过全连接\n",
    "    多项选择将文本分别与每个答案组合输入GPT，再分别经过全连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    将一句话中的词按百分比进行mask处理，大概率代换成mask标签，小概率代换成同句子的其他词或者保持不变，这样对于句子中的词可以看到其前后双向的词，而有的词会被mask处理以避免直接复制的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.句子中每个词的token embeddings\n",
    "    2.每句话各自的segment embeddings，同一句话中每个词的segment embeddings相同\n",
    "    3.所有词各自的position embeddings，每个词都不一样，随机初始化并学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    对于分类任务，将句子输入训练好的bert模型得到分类的输出；对于问答任务，将问题与文本段落输入，得到答案句子的开始与结束位置；对于实体识别任务，将句子输入，得到每个词所属的词类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    GPT为单向masked的Transformer，每个词只能与之前的词联系；Bert为双向masked的Transformer，每个词可以与前后的词联系，按百分比随机mask；GPT2相较于GPT有更多的参数，使用更多的数据进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
